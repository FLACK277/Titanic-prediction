{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a64218-1fd2-4fae-aa95-20d7cee8ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:152: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:152: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\rawat\\AppData\\Local\\Temp\\ipykernel_37984\\2105581048.py:152: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  X['Title'] = X['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset with 891 rows and 12 columns\n",
      "\n",
      "Dataset Overview:\n",
      "Shape: (891, 12)\n",
      "\n",
      "Data Types:\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "Missing Values Percentage: PassengerId     0.000000\n",
      "Survived        0.000000\n",
      "Pclass          0.000000\n",
      "Name            0.000000\n",
      "Sex             0.000000\n",
      "Age            19.865320\n",
      "SibSp           0.000000\n",
      "Parch           0.000000\n",
      "Ticket          0.000000\n",
      "Fare            0.000000\n",
      "Cabin          77.104377\n",
      "Embarked        0.224467\n",
      "dtype: float64\n",
      "\n",
      "Summary Statistics:\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "Target Variable Distribution:\n",
      "Survived: 342 (38.38%)\n",
      "Did not survive: 549 (61.62%)\n",
      "\n",
      "Survival by Sex:\n",
      "Sex\n",
      "female    0.742038\n",
      "male      0.188908\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Survival by Passenger Class:\n",
      "Pclass\n",
      "1    0.629630\n",
      "2    0.472826\n",
      "3    0.242363\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Survival by Age Group:\n",
      "AgeGroup\n",
      "Child          0.579710\n",
      "Teenager       0.428571\n",
      "Adult          0.400000\n",
      "Young Adult    0.382682\n",
      "Senior         0.227273\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Survival by Family Size:\n",
      "FamilySize\n",
      "4     0.724138\n",
      "3     0.578431\n",
      "2     0.552795\n",
      "7     0.333333\n",
      "1     0.303538\n",
      "5     0.200000\n",
      "6     0.136364\n",
      "8     0.000000\n",
      "11    0.000000\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Survival by Port of Embarkation:\n",
      "Embarked\n",
      "C    0.553571\n",
      "Q    0.389610\n",
      "S    0.336957\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "===== END OF EDA =====\n",
      "\n",
      "===== DATA PREPROCESSING =====\n",
      "Target variable 'Survived' separated with 342 positive cases out of 891 total\n",
      "\n",
      "Performing feature engineering...\n",
      "Created 'Title' feature with categories: ['Mr' 'Mrs' 'Miss' 'Master' 'Rare']\n",
      "Created 'FamilySize' and 'IsAlone' features\n",
      "Created 'CabinLetter' and 'HasCabin' features\n",
      "Created 'FarePerPerson' feature\n",
      "\n",
      "Numerical features: ['Age', 'Fare', 'FamilySize', 'FarePerPerson']\n",
      "Categorical features: ['Pclass', 'Sex', 'Embarked', 'Title', 'IsAlone', 'CabinLetter', 'HasCabin']\n",
      "Features to drop: ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin']\n",
      "\n",
      "Splitting data into 80.0% training and 20.0% testing sets...\n",
      "Training set: 712 samples\n",
      "Testing set: 179 samples\n",
      "\n",
      "Training LogisticRegression...\n",
      "\n",
      "LogisticRegression Performance:\n",
      "Accuracy: 0.8436\n",
      "Precision: 0.8154\n",
      "Recall: 0.7681\n",
      "F1-Score: 0.7910\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88       110\n",
      "           1       0.82      0.77      0.79        69\n",
      "\n",
      "    accuracy                           0.84       179\n",
      "   macro avg       0.84      0.83      0.83       179\n",
      "weighted avg       0.84      0.84      0.84       179\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98 12]\n",
      " [16 53]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rawat\\AppData\\Local\\Temp\\ipykernel_37984\\2105581048.py:90: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  age_survival = data.groupby('AgeGroup')['Survived'].mean().sort_values(ascending=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation scores: [0.77622378 0.8041958  0.85211268 0.84507042 0.83098592]\n",
      "Mean CV accuracy: 0.8217\n",
      "Standard deviation of CV accuracy: 0.0280\n",
      "\n",
      "Training RandomForest...\n",
      "\n",
      "RandomForest Performance:\n",
      "Accuracy: 0.7821\n",
      "Precision: 0.7344\n",
      "Recall: 0.6812\n",
      "F1-Score: 0.7068\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       110\n",
      "           1       0.73      0.68      0.71        69\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.77      0.76      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93 17]\n",
      " [22 47]]\n",
      "\n",
      "Cross-validation scores: [0.76923077 0.72727273 0.82394366 0.82394366 0.85211268]\n",
      "Mean CV accuracy: 0.7993\n",
      "Standard deviation of CV accuracy: 0.0449\n",
      "\n",
      "Training GradientBoosting...\n",
      "\n",
      "GradientBoosting Performance:\n",
      "Accuracy: 0.8156\n",
      "Precision: 0.7903\n",
      "Recall: 0.7101\n",
      "F1-Score: 0.7481\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85       110\n",
      "           1       0.79      0.71      0.75        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.82      0.81       179\n",
      "\n",
      "Confusion Matrix:\n",
      "[[97 13]\n",
      " [20 49]]\n",
      "\n",
      "Cross-validation scores: [0.78321678 0.78321678 0.87323944 0.85915493 0.83098592]\n",
      "Mean CV accuracy: 0.8260\n",
      "Standard deviation of CV accuracy: 0.0375\n",
      "\n",
      "Best Model: LogisticRegression\n",
      "Best Model Accuracy: 0.8436\n",
      "\n",
      "===== END OF MODEL TRAINING AND EVALUATION =====\n",
      "Tuning LogisticRegression hyperparameters: C, solver, penalty\n",
      "\n",
      "Performing grid search (this may take a while)...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "Best Parameters: {'classifier__C': 1, 'classifier__penalty': 'l2', 'classifier__solver': 'saga'}\n",
      "Best Cross-validation Score: 0.8217\n",
      "\n",
      "Tuned Model Performance on Test Set:\n",
      "Accuracy: 0.8436\n",
      "Precision: 0.8154\n",
      "Recall: 0.7681\n",
      "F1-Score: 0.7910\n",
      "\n",
      "Hyperparameter tuning did not improve model performance. Using original model.\n",
      "\n",
      "===== END OF HYPERPARAMETER TUNING =====\n",
      "\n",
      "===== FEATURE IMPORTANCE ANALYSIS =====\n",
      "\n",
      "Top 10 Positive Coefficients:\n",
      "               Feature  Coefficient\n",
      "12   cat__Title_Master     1.165867\n",
      "23  cat__CabinLetter_E     0.806667\n",
      "7      cat__Sex_female     0.785626\n",
      "22  cat__CabinLetter_D     0.708162\n",
      "15      cat__Title_Mrs     0.614638\n",
      "4        cat__Pclass_1     0.466691\n",
      "28     cat__HasCabin_1     0.378531\n",
      "10     cat__Embarked_Q     0.285089\n",
      "5        cat__Pclass_2     0.194981\n",
      "1            num__Fare     0.174881\n",
      "\n",
      "Top 10 Negative Coefficients:\n",
      "               Feature  Coefficient\n",
      "26  cat__CabinLetter_T    -0.287582\n",
      "16     cat__Title_Rare    -0.352231\n",
      "11     cat__Embarked_S    -0.354400\n",
      "27     cat__HasCabin_0    -0.370896\n",
      "0             num__Age    -0.405171\n",
      "2      num__FamilySize    -0.636253\n",
      "6        cat__Pclass_3    -0.654037\n",
      "25  cat__CabinLetter_G    -0.743588\n",
      "8        cat__Sex_male    -0.777991\n",
      "14       cat__Title_Mr    -1.297813\n",
      "\n",
      "===== FINAL MODEL EVALUATION =====\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88       110\n",
      "           1       0.82      0.77      0.79        69\n",
      "\n",
      "    accuracy                           0.84       179\n",
      "   macro avg       0.84      0.83      0.83       179\n",
      "weighted avg       0.84      0.84      0.84       179\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[98 12]\n",
      " [16 53]]\n",
      "True Positives: 53\n",
      "True Negatives: 98\n",
      "False Positives: 12\n",
      "False Negatives: 16\n",
      "\n",
      "Additional Metrics:\n",
      "Specificity (True Negative Rate): 0.8909\n",
      "Negative Predictive Value: 0.8596\n",
      "\n",
      "===== END OF FINAL MODEL EVALUATION =====\n",
      "\n",
      "================================================================================\n",
      "                 TITANIC SURVIVAL PREDICTION PROJECT SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. DATA PREPROCESSING:\n",
      "   - Handled missing values in Age, Cabin, and Embarked columns\n",
      "   - Extracted title from passenger names\n",
      "   - Created family size feature and 'IsAlone' indicator\n",
      "   - Created 'HasCabin' feature based on cabin information availability\n",
      "   - Normalized fare by family size\n",
      "   - Applied one-hot encoding to categorical variables\n",
      "   - Scaled numerical features\n",
      "\n",
      "2. MODEL DEVELOPMENT:\n",
      "   - LogisticRegression: 0.8436 accuracy\n",
      "   - RandomForest: 0.7821 accuracy\n",
      "   - GradientBoosting: 0.8156 accuracy\n",
      "   - Selected LogisticRegression as the best base model\n",
      "   - Final model accuracy after tuning: 0.8436\n",
      "\n",
      "3. KEY FINDINGS:\n",
      "   - Gender was a strong predictor (women had higher survival rates)\n",
      "   - Passenger class was important (higher classes had better survival rates)\n",
      "   - Age was a factor (children had better survival chances)\n",
      "   - Cabin information indicated social status and proximity to lifeboats\n",
      "   - Family size affected survival (very large families had lower survival rates)\n",
      "\n",
      "4. RECOMMENDATIONS FOR FURTHER IMPROVEMENT:\n",
      "   - Collect more data if available\n",
      "   - Try more advanced feature engineering:\n",
      "     * Deck information from cabin numbers\n",
      "     * More sophisticated family grouping\n",
      "     * Better handling of rare categories\n",
      "   - Experiment with ensemble methods (voting, stacking)\n",
      "   - Try advanced models like XGBoost or neural networks\n",
      "   - Perform more extensive hyperparameter tuning\n",
      "\n",
      "5. NEXT STEPS:\n",
      "   - Deploy the model via a simple web interface\n",
      "   - Create a data pipeline for new predictions\n",
      "   - Document the model for other data scientists\n",
      "   - Prepare a presentation for stakeholders\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "             INSTRUCTIONS FOR USING THE MODEL WITH NEW DATA\n",
      "================================================================================\n",
      "\n",
      "To use this model with new passenger data:\n",
      "\n",
      "1. Ensure your data contains the same features as the training data:\n",
      "   - PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked\n",
      "\n",
      "2. Load your new data:\n",
      "   ```python\n",
      "   new_data = pd.read_csv('new_passengers.csv')\n",
      "   ```\n",
      "\n",
      "3. Preprocess the new data using the same preprocessor:\n",
      "   ```python\n",
      "   X_new = preprocess_data(new_data)[0]\n",
      "   ```\n",
      "\n",
      "4. Make predictions:\n",
      "   ```python\n",
      "   predictions = tuned_model.predict(X_new)\n",
      "   prediction_probs = tuned_model.predict_proba(X_new)[:, 1]\n",
      "   ```\n",
      "\n",
      "5. Format and save the predictions:\n",
      "   ```python\n",
      "   results = pd.DataFrame({\n",
      "       'PassengerId': new_data['PassengerId'],\n",
      "       'Survived': predictions,\n",
      "       'Survival_Probability': prediction_probs\n",
      "   })\n",
      "   results.to_csv('prediction_results.csv', index=False)\n",
      "   ```\n",
      "\n",
      "Note: For consistent results, ensure that your new data follows the same format\n",
      "and has the same column names as the original training data.\n",
      "    \n",
      "================================================================================\n",
      "\n",
      "Project complete. The model is ready for use!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data \n",
    "def load_titanic_data(train_path='Downloads/titanic.csv'):\n",
    "    \"\"\"\n",
    "    Loads the Titanic dataset from CSV file.\n",
    "    Returns the dataset as a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(train_path)\n",
    "        print(f\"Successfully loaded dataset with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not find the dataset file at {train_path}\")\n",
    "        print(\"Creating a dummy dataset structure for demonstration purposes\")\n",
    "        \n",
    "        # Creating dummy dataset\n",
    "        dummy_data = pd.DataFrame({\n",
    "            'PassengerId': range(1, 101),\n",
    "            'Survived': np.random.choice([0, 1], size=100),\n",
    "            'Pclass': np.random.choice([1, 2, 3], size=100),\n",
    "            'Name': ['Person ' + str(i) for i in range(1, 101)],\n",
    "            'Sex': np.random.choice(['male', 'female'], size=100),\n",
    "            'Age': np.random.normal(30, 14, 100),\n",
    "            'SibSp': np.random.choice(range(0, 5), size=100),\n",
    "            'Parch': np.random.choice(range(0, 5), size=100),\n",
    "            'Ticket': ['TICKET_' + str(i) for i in range(1, 101)],\n",
    "            'Fare': np.random.normal(30, 50, 100),\n",
    "            'Cabin': [np.random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G']) + str(np.random.randint(1, 100)) if np.random.random() > 0.3 else None for _ in range(100)],\n",
    "            'Embarked': np.random.choice(['C', 'Q', 'S', None], size=100, p=[0.3, 0.1, 0.58, 0.02])\n",
    "        })\n",
    "        \n",
    "        # Adding missing values to the dataset\n",
    "        dummy_data.loc[np.random.choice(dummy_data.index, 20), 'Age'] = None\n",
    "        \n",
    "        return dummy_data\n",
    "\n",
    "# Load the dataset\n",
    "titanic_data = load_titanic_data()\n",
    "\n",
    "# Exploratory Data Analysis Function\n",
    "def explore_data(data):\n",
    "    \"\"\"Perform exploratory data analysis on the dataset.\"\"\"\n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(\"\\nData Types:\")\n",
    "    print(data.dtypes)\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(data.isnull().sum())\n",
    "    print(f\"Missing Values Percentage: {data.isnull().sum() / len(data) * 100}\")\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    if 'Survived' in data.columns:\n",
    "        print(\"\\nTarget Variable Distribution:\")\n",
    "        survival_counts = data['Survived'].value_counts()\n",
    "        print(f\"Survived: {survival_counts[1]} ({survival_counts[1]/len(data)*100:.2f}%)\")\n",
    "        print(f\"Did not survive: {survival_counts[0]} ({survival_counts[0]/len(data)*100:.2f}%)\")\n",
    "        \n",
    "        print(\"\\nSurvival by Sex:\")\n",
    "        sex_survival = data.groupby('Sex')['Survived'].mean().sort_values(ascending=False)\n",
    "        print(sex_survival)\n",
    "        \n",
    "        print(\"\\nSurvival by Passenger Class:\")\n",
    "        class_survival = data.groupby('Pclass')['Survived'].mean().sort_values(ascending=False)\n",
    "        print(class_survival)\n",
    "        \n",
    "        # Create age groups for analysis\n",
    "        data['AgeGroup'] = pd.cut(data['Age'], \n",
    "                                  bins=[0, 12, 18, 35, 60, 100], \n",
    "                                  labels=['Child', 'Teenager', 'Young Adult', 'Adult', 'Senior'])\n",
    "        \n",
    "        print(\"\\nSurvival by Age Group:\")\n",
    "        if data['AgeGroup'].notna().any():\n",
    "            age_survival = data.groupby('AgeGroup')['Survived'].mean().sort_values(ascending=False)\n",
    "            print(age_survival)\n",
    "        else:\n",
    "            print(\"No age data available for this analysis.\")\n",
    "        \n",
    "        # Calculate family size\n",
    "        data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "        \n",
    "        print(\"\\nSurvival by Family Size:\")\n",
    "        family_survival = data.groupby('FamilySize')['Survived'].mean().sort_values(ascending=False)\n",
    "        print(family_survival)\n",
    "        \n",
    "        # Check if Embarked data is available\n",
    "        if 'Embarked' in data.columns and data['Embarked'].notna().any():\n",
    "            print(\"\\nSurvival by Port of Embarkation:\")\n",
    "            embark_survival = data.groupby('Embarked')['Survived'].mean().sort_values(ascending=False)\n",
    "            print(embark_survival)\n",
    "    \n",
    "    # Cleaning up temporary columns\n",
    "    if 'AgeGroup' in data.columns:\n",
    "        data.drop('AgeGroup', axis=1, inplace=True)\n",
    "    if 'FamilySize' in data.columns:\n",
    "        data.drop('FamilySize', axis=1, inplace=True)\n",
    "        \n",
    "    print(\"\\n===== END OF EDA =====\")\n",
    "\n",
    "# Perform exploratory data analysis\n",
    "explore_data(titanic_data)\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess the Titanic dataset for machine learning.\n",
    "    Handles missing values, performs feature engineering, and prepares data for modeling.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing Titanic dataset\n",
    "        \n",
    "    Returns:\n",
    "        X: Features DataFrame\n",
    "        y: Target Series (if available)\n",
    "        feature_names: List of feature names after preprocessing\n",
    "    \"\"\"\n",
    "    print(\"\\n===== DATA PREPROCESSING =====\")\n",
    "    \n",
    "    # Created a copy of the data to avoid modifying the original\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Separated features and target if target is available\n",
    "    if 'Survived' in df.columns:\n",
    "        y = df['Survived']\n",
    "        X = df.drop('Survived', axis=1)\n",
    "        print(f\"Target variable 'Survived' separated with {y.sum()} positive cases out of {len(y)} total\")\n",
    "    else:\n",
    "        y = None\n",
    "        X = df\n",
    "        print(\"No target variable 'Survived' found in the dataset\")\n",
    "    \n",
    "    # Feature Engineering\n",
    "    print(\"\\nPerforming feature engineering...\")\n",
    "    \n",
    "    # 1. Extract titles from names\n",
    "    X['Title'] = X['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    # Group rare titles\n",
    "    title_mapping = {\n",
    "        \"Mr\": \"Mr\",\n",
    "        \"Miss\": \"Miss\",\n",
    "        \"Mrs\": \"Mrs\",\n",
    "        \"Master\": \"Master\",\n",
    "        \"Dr\": \"Rare\",\n",
    "        \"Rev\": \"Rare\",\n",
    "        \"Col\": \"Rare\",\n",
    "        \"Major\": \"Rare\",\n",
    "        \"Mlle\": \"Miss\",\n",
    "        \"Countess\": \"Rare\",\n",
    "        \"Ms\": \"Miss\",\n",
    "        \"Lady\": \"Rare\",\n",
    "        \"Jonkheer\": \"Rare\",\n",
    "        \"Don\": \"Rare\",\n",
    "        \"Dona\": \"Rare\",\n",
    "        \"Mme\": \"Mrs\",\n",
    "        \"Capt\": \"Rare\",\n",
    "        \"Sir\": \"Rare\"\n",
    "    }\n",
    "    \n",
    "    X['Title'] = X['Title'].map(lambda x: title_mapping.get(x, \"Rare\"))\n",
    "    print(f\"Created 'Title' feature with categories: {X['Title'].unique()}\")\n",
    "    \n",
    "    # 2.Created family size and is_alone features\n",
    "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n",
    "    X['IsAlone'] = (X['FamilySize'] == 1).astype(int)\n",
    "    print(f\"Created 'FamilySize' and 'IsAlone' features\")\n",
    "    \n",
    "    # 3.Extract cabin information if available\n",
    "    if 'Cabin' in X.columns:\n",
    "        X['CabinLetter'] = X['Cabin'].str[0]\n",
    "        X['HasCabin'] = (~X['Cabin'].isna()).astype(int)\n",
    "        print(f\"Created 'CabinLetter' and 'HasCabin' features\")\n",
    "    \n",
    "    # 4.Create fare per person feature\n",
    "    X['FarePerPerson'] = X['Fare'] / X['FamilySize']\n",
    "    print(f\"Created 'FarePerPerson' feature\")\n",
    "    \n",
    "    # Identify numerical and categorical features for preprocessing\n",
    "    numerical_features = ['Age', 'Fare', 'FamilySize', 'FarePerPerson']\n",
    "    categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'IsAlone']\n",
    "    \n",
    "    if 'CabinLetter' in X.columns:\n",
    "        categorical_features.append('CabinLetter')\n",
    "    if 'HasCabin' in X.columns:\n",
    "        categorical_features.append('HasCabin')\n",
    "    \n",
    "    # Features to drop\n",
    "    drop_features = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin']\n",
    "    \n",
    "    #preprocessing steps for numerical features\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Define the preprocessing steps for categorical features\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'  \n",
    "    )\n",
    "    \n",
    "    print(f\"\\nNumerical features: {numerical_features}\")\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    print(f\"Features to drop: {drop_features}\")\n",
    "        \n",
    "    return X, y, preprocessor\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, preprocessor = preprocess_data(titanic_data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        X: Features DataFrame\n",
    "        y: Target Series\n",
    "        test_size: Proportion of data to use for testing\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    print(f\"\\nSplitting data into {100-test_size*100}% training and {test_size*100}% testing sets...\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split the data if target is available\n",
    "if y is not None:\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "else:\n",
    "    print(\"\\nWARNING: No target variable available, cannot split data.\")\n",
    "    X_train, X_test, y_train, y_test = X, None, y, None\n",
    "\n",
    "# Model Selection and Training\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models on the Titanic dataset.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_test: Testing features\n",
    "        y_test: Testing target\n",
    "        preprocessor: Data preprocessing ColumnTransformer\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trained models and their performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define models to try\n",
    "    models = {\n",
    "        'LogisticRegression': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "        ]),\n",
    "        'RandomForest': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'GradientBoosting': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{name} Performance:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Display confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV accuracy: {cv_scores.mean():.4f}\")\n",
    "        print(f\"Standard deviation of CV accuracy: {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Find the best model\n",
    "    best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "    best_model = results[best_model_name]\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    print(f\"Best Model Accuracy: {best_model['accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"\\n===== END OF MODEL TRAINING AND EVALUATION =====\")\n",
    "    \n",
    "    return results, best_model_name\n",
    "\n",
    "# Train and evaluate models if target is available\n",
    "if y_train is not None and y_test is not None:\n",
    "    model_results, best_model_name = train_and_evaluate_models(X_train, y_train, X_test, y_test, preprocessor)\n",
    "    best_model = model_results[best_model_name]['model']\n",
    "else:\n",
    "    print(\"\\nWARNING: No target variable available, cannot train models.\")\n",
    "    model_results, best_model_name, best_model = None, None, None\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "def tune_hyperparameters(best_model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning on the best model.\n",
    "    \n",
    "    Args:\n",
    "        best_model: Best performing model\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_test: Testing features\n",
    "        y_test: Testing target\n",
    "        \n",
    "    Returns:\n",
    "        Tuned model and its performance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the classifier name and instance\n",
    "    classifier_name = best_model.steps[-1][0]\n",
    "    classifier = best_model.steps[-1][1]\n",
    "    \n",
    "    # Define parameter grid based on the classifier type\n",
    "    param_grid = {}\n",
    "    \n",
    "    if isinstance(classifier, LogisticRegression):\n",
    "        param_grid = {\n",
    "            f'{classifier_name}__C': [0.01, 0.1, 1, 10, 100],\n",
    "            f'{classifier_name}__solver': ['liblinear', 'saga'],\n",
    "            f'{classifier_name}__penalty': ['l1', 'l2']\n",
    "        }\n",
    "        print(\"Tuning LogisticRegression hyperparameters: C, solver, penalty\")\n",
    "    \n",
    "    elif isinstance(classifier, RandomForestClassifier):\n",
    "        param_grid = {\n",
    "            f'{classifier_name}__n_estimators': [50, 100, 200],\n",
    "            f'{classifier_name}__max_depth': [None, 5, 10, 15],\n",
    "            f'{classifier_name}__min_samples_split': [2, 5, 10],\n",
    "            f'{classifier_name}__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        print(\"Tuning RandomForest hyperparameters: n_estimators, max_depth, min_samples_split, min_samples_leaf\")\n",
    "    \n",
    "    elif isinstance(classifier, GradientBoostingClassifier):\n",
    "        param_grid = {\n",
    "            f'{classifier_name}__n_estimators': [50, 100, 200],\n",
    "            f'{classifier_name}__learning_rate': [0.01, 0.1, 0.2],\n",
    "            f'{classifier_name}__max_depth': [3, 5, 7],\n",
    "            f'{classifier_name}__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        print(\"Tuning GradientBoosting hyperparameters: n_estimators, learning_rate, max_depth, min_samples_split\")\n",
    "    \n",
    "    # Create GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=best_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    print(\"\\nPerforming grid search (this may take a while)...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters and best estimator\n",
    "    print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-validation Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate the tuned model on the test set\n",
    "    tuned_model = grid_search.best_estimator_\n",
    "    y_pred = tuned_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nTuned Model Performance on Test Set:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Check if tuning improved performance\n",
    "    original_accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
    "    \n",
    "    if accuracy > original_accuracy:\n",
    "        print(f\"\\nHyperparameter tuning improved accuracy by {(accuracy - original_accuracy) * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\nHyperparameter tuning did not improve model performance. Using original model.\")\n",
    "        tuned_model = best_model\n",
    "    \n",
    "    print(\"\\n===== END OF HYPERPARAMETER TUNING =====\")\n",
    "    \n",
    "    return tuned_model, accuracy, precision, recall, f1\n",
    "\n",
    "# Tune hyperparameters if best model is available\n",
    "if best_model is not None and y_train is not None and y_test is not None:\n",
    "    tuned_model, final_accuracy, final_precision, final_recall, final_f1 = tune_hyperparameters(\n",
    "        best_model, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nWARNING: No best model available, cannot perform hyperparameter tuning.\")\n",
    "    tuned_model, final_accuracy, final_precision, final_recall, final_f1 = None, None, None, None, None\n",
    "\n",
    "# Feature Importance Analysis\n",
    "def analyze_feature_importance(model, X):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X: Feature DataFrame (for column names)\n",
    "    \"\"\"\n",
    "    print(\"\\n===== FEATURE IMPORTANCE ANALYSIS =====\")\n",
    "    \n",
    "    # Get the classifier\n",
    "    classifier = model.named_steps['classifier']\n",
    "    \n",
    "    # Check if the classifier has feature_importances_ attribute\n",
    "    if hasattr(classifier, 'feature_importances_'):\n",
    "        try:\n",
    "            # For newer scikit-learn versions\n",
    "            feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        except:\n",
    "            # Fallback for older versions or if the above fails\n",
    "            feature_names = [f\"feature_{i}\" for i in range(len(classifier.feature_importances_))]\n",
    "        \n",
    "        # Create a DataFrame of feature importances\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': classifier.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Print the top features\n",
    "        print(\"\\nTop 15 Most Important Features:\")\n",
    "        print(importance_df.head(15))\n",
    "        \n",
    "        # Group features by type (numerical vs categorical)\n",
    "        if len(feature_names) > 0:\n",
    "            feature_types = []\n",
    "            for name in feature_names:\n",
    "                if name.startswith('num__'):\n",
    "                    feature_types.append('Numerical')\n",
    "                elif name.startswith('cat__'):\n",
    "                    feature_types.append('Categorical')\n",
    "                else:\n",
    "                    feature_types.append('Other')\n",
    "            \n",
    "            importance_df['Type'] = feature_types\n",
    "            \n",
    "            # Average importance by feature type\n",
    "            type_importance = importance_df.groupby('Type')['Importance'].mean().sort_values(ascending=False)\n",
    "            print(\"\\nAverage Importance by Feature Type:\")\n",
    "            print(type_importance)\n",
    "    \n",
    "    elif hasattr(classifier, 'coef_'):\n",
    "        # For linear models like LogisticRegression\n",
    "        try:\n",
    "            # For newer scikit-learn versions\n",
    "            feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        except:\n",
    "            # Fallback for older versions or if the above fails\n",
    "            feature_names = [f\"feature_{i}\" for i in range(classifier.coef_.shape[1])]\n",
    "        \n",
    "        # Created a DataFrame of coefficients\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': classifier.coef_[0]\n",
    "        }).sort_values('Coefficient', ascending=False)\n",
    "        \n",
    "        # Print the top positive and negative coefficients\n",
    "        print(\"\\nTop 10 Positive Coefficients:\")\n",
    "        print(coef_df.head(10))\n",
    "        \n",
    "        print(\"\\nTop 10 Negative Coefficients:\")\n",
    "        print(coef_df.tail(10))\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nThis model doesn't provide feature importance information.\")\n",
    "    \n",
    "\n",
    "# Analyze feature importance if tuned model is available\n",
    "if tuned_model is not None:\n",
    "    analyze_feature_importance(tuned_model, X)\n",
    "else:\n",
    "    print(\"\\nWARNING: No tuned model available, cannot analyze feature importance.\")\n",
    "\n",
    "# Final Model Evaluation\n",
    "def evaluate_final_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform final evaluation of the tuned model.\n",
    "    \n",
    "    Args:\n",
    "        model: Tuned model\n",
    "        X_test: Test features\n",
    "        y_test: Test target\n",
    "    \"\"\"\n",
    "    print(\"\\n===== FINAL MODEL EVALUATION =====\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"True Positives: {cm[1][1]}\")\n",
    "    print(f\"True Negatives: {cm[0][0]}\")\n",
    "    print(f\"False Positives: {cm[0][1]}\")\n",
    "    print(f\"False Negatives: {cm[1][0]}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Specificity (true negative rate)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Negative predictive value\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "    print(f\"Negative Predictive Value: {npv:.4f}\")\n",
    "    \n",
    "    print(\"\\n===== END OF FINAL MODEL EVALUATION =====\")\n",
    "\n",
    "# Evaluate final model if available\n",
    "if tuned_model is not None and y_test is not None:\n",
    "    evaluate_final_model(tuned_model, X_test, y_test)\n",
    "else:\n",
    "    print(\"\\nWARNING: No tuned model or test data available, cannot perform final evaluation.\")\n",
    "\n",
    "# Project Summary\n",
    "def summarize_project(model_results=None, best_model_name=None, final_accuracy=None):\n",
    "    \"\"\"\n",
    "    Summarize the Titanic survival prediction project.\n",
    "    \n",
    "    Args:\n",
    "        model_results: Dictionary of model results\n",
    "        best_model_name: Name of the best model\n",
    "        final_accuracy: Final accuracy after tuning\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"                 TITANIC SURVIVAL PREDICTION PROJECT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. DATA PREPROCESSING:\")\n",
    "    print(\"   - Handled missing values in Age, Cabin, and Embarked columns\")\n",
    "    print(\"   - Extracted title from passenger names\")\n",
    "    print(\"   - Created family size feature and 'IsAlone' indicator\")\n",
    "    print(\"   - Created 'HasCabin' feature based on cabin information availability\")\n",
    "    print(\"   - Normalized fare by family size\")\n",
    "    print(\"   - Applied one-hot encoding to categorical variables\")\n",
    "    print(\"   - Scaled numerical features\")\n",
    "    \n",
    "    print(\"\\n2. MODEL DEVELOPMENT:\")\n",
    "    if model_results:\n",
    "        model_accuracies = {name: results['accuracy'] for name, results in model_results.items()}\n",
    "        for name, accuracy in model_accuracies.items():\n",
    "            print(f\"   - {name}: {accuracy:.4f} accuracy\")\n",
    "        \n",
    "        if best_model_name:\n",
    "            print(f\"   - Selected {best_model_name} as the best base model\")\n",
    "            \n",
    "        if final_accuracy:\n",
    "            print(f\"   - Final model accuracy after tuning: {final_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"   - No model results available\")\n",
    "    \n",
    "    print(\"\\n3. KEY FINDINGS:\")\n",
    "    print(\"   - Gender was a strong predictor (women had higher survival rates)\")\n",
    "    print(\"   - Passenger class was important (higher classes had better survival rates)\")\n",
    "    print(\"   - Age was a factor (children had better survival chances)\")\n",
    "    print(\"   - Cabin information indicated social status and proximity to lifeboats\")\n",
    "    print(\"   - Family size affected survival (very large families had lower survival rates)\")\n",
    "    \n",
    "    print(\"\\n4. RECOMMENDATIONS FOR FURTHER IMPROVEMENT:\")\n",
    "    print(\"   - Collect more data if available\")\n",
    "    print(\"   - Try more advanced feature engineering:\")\n",
    "    print(\"     * Deck information from cabin numbers\")\n",
    "    print(\"     * More sophisticated family grouping\")\n",
    "    print(\"     * Better handling of rare categories\")\n",
    "    print(\"   - Experiment with ensemble methods (voting, stacking)\")\n",
    "    print(\"   - Try advanced models like XGBoost or neural networks\")\n",
    "    print(\"   - Perform more extensive hyperparameter tuning\")\n",
    "    \n",
    "    print(\"\\n5. NEXT STEPS:\")\n",
    "    print(\"   - Deploy the model via a simple web interface\")\n",
    "    print(\"   - Create a data pipeline for new predictions\")\n",
    "    print(\"   - Document the model for other data scientists\")\n",
    "    print(\"   - Prepare a presentation for stakeholders\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Print project summary\n",
    "summarize_project(model_results, best_model_name, final_accuracy)\n",
    "\n",
    "# Instructions for using the model with new data\n",
    "def provide_usage_instructions():\n",
    "    \"\"\"Provide instructions for using the trained model with new data.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"             INSTRUCTIONS FOR USING THE MODEL WITH NEW DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "To use this model with new passenger data:\n",
    "\n",
    "1. Ensure your data contains the same features as the training data:\n",
    "   - PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked\n",
    "\n",
    "2. Load your new data:\n",
    "   ```python\n",
    "   new_data = pd.read_csv('new_passengers.csv')\n",
    "   ```\n",
    "\n",
    "3. Preprocess the new data using the same preprocessor:\n",
    "   ```python\n",
    "   X_new = preprocess_data(new_data)[0]\n",
    "   ```\n",
    "\n",
    "4. Make predictions:\n",
    "   ```python\n",
    "   predictions = tuned_model.predict(X_new)\n",
    "   prediction_probs = tuned_model.predict_proba(X_new)[:, 1]\n",
    "   ```\n",
    "\n",
    "5. Format and save the predictions:\n",
    "   ```python\n",
    "   results = pd.DataFrame({\n",
    "       'PassengerId': new_data['PassengerId'],\n",
    "       'Survived': predictions,\n",
    "       'Survival_Probability': prediction_probs\n",
    "   })\n",
    "   results.to_csv('prediction_results.csv', index=False)\n",
    "   ```\n",
    "\n",
    "Note: For consistent results, ensure that your new data follows the same format\n",
    "and has the same column names as the original training data.\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Provide usage instructions\n",
    "provide_usage_instructions()\n",
    "\n",
    "print(\"\\nProject complete. The model is ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14250650-6e2a-4022-a783-f7a4804e5f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
